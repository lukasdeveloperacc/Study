{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dafb67ef",
   "metadata": {},
   "source": [
    "## First Letcure : START -> Generate -> END\n",
    "## Next : START -> Retrieve -> Generate -> END\n",
    "### Document : 소득세법\n",
    "- 우리말 문서는 word document 추천\n",
    "- 이번 강의에서는 PDF\n",
    "\n",
    "### 전처리\n",
    "- 우리말 PDF의 문제 : 줄 바꿈이 일어났을 때 예를들어 \"법인세\"라는 단어를 -> \"법\", \"인세\"로 인식하게 되는 문제가 있다.\n",
    "  - 임베딩 시 정확도가 떨어지는 문제 따라서 docs를 사용하는 것을 추천\n",
    "  - 영어는 단어가 바뀔때마다 줄바꿈이됨\n",
    "- RAG\n",
    "  - 문서를 통짜로 주면 Token 소모가 매우 큼(고비용) + 시간\n",
    "  - 필요 없는 문서도 넘어가므로 할루시네이션 문제 증가\n",
    "  - 문서를 적당한 사이즈로 쪼갠 다음에 LLM에 전달하는 것\n",
    "- 기본적으로 제공하는 PDF parser는 이미지를 파싱할 수 없다\n",
    "  - 1. ChatGPT 같은 것 사용해서 전처리 ? -> 일일이 언제 다함 ?\n",
    "  - 2. zerox -> LLM 활용한 OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c3e522c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luke/Study-2/langchain/kang/.venv/lib/python3.13/site-packages/pypdf/generic/_base.py:892: RuntimeWarning: coroutine 'main' was never awaited\n",
      "  return NameObject(ret)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCancelledError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m loader = PyPDFLoader(pdf_file_path)\n\u001b[32m      5\u001b[39m pages = []\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m loader.alazy_load():\n\u001b[32m      7\u001b[39m     pages.append(page)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Study-2/langchain/kang/.venv/lib/python3.13/site-packages/langchain_core/document_loaders/base.py:83\u001b[39m, in \u001b[36mBaseLoader.alazy_load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     81\u001b[39m done = \u001b[38;5;28mobject\u001b[39m()\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m     doc = \u001b[38;5;28;01mawait\u001b[39;00m run_in_executor(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mnext\u001b[39m, iterator, done)\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m doc \u001b[38;5;129;01mis\u001b[39;00m done:\n\u001b[32m     85\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Study-2/langchain/kang/.venv/lib/python3.13/site-packages/langchain_core/runnables/config.py:617\u001b[39m, in \u001b[36mrun_in_executor\u001b[39m\u001b[34m(executor_or_config, func, *args, **kwargs)\u001b[39m\n\u001b[32m    613\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m executor_or_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(executor_or_config, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;66;03m# Use default executor with context copied from current context\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio.get_running_loop().run_in_executor(\n\u001b[32m    618\u001b[39m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    619\u001b[39m         cast(\u001b[33m\"\u001b[39m\u001b[33mCallable[..., T]\u001b[39m\u001b[33m\"\u001b[39m, partial(copy_context().run, wrapper)),\n\u001b[32m    620\u001b[39m     )\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio.get_running_loop().run_in_executor(executor_or_config, wrapper)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.3-linux-x86_64-gnu/lib/python3.13/asyncio/futures.py:286\u001b[39m, in \u001b[36mFuture.__await__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done():\n\u001b[32m    285\u001b[39m     \u001b[38;5;28mself\u001b[39m._asyncio_future_blocking = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m  \u001b[38;5;66;03m# This tells Task to wait for completion.\u001b[39;00m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done():\n\u001b[32m    288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mawait wasn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt used with future\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mCancelledError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf_file_path = \"./docs/income_tax.pdf\"\n",
    "loader = PyPDFLoader(pdf_file_path)\n",
    "pages = []\n",
    "async for page in loader.alazy_load():\n",
    "    pages.append(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1d4b17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'iText 2.1.7 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-08-05T15:03:16+09:00', 'moddate': '2025-08-05T15:03:16+09:00', 'source': './docs/income_tax.pdf', 'total_pages': 130, 'page': 35, 'page_label': '36'}, page_content='법제처                                                            36                                                       국가법령정보센터\\n소득세법\\n[본조신설 2022. 12. 31.]\\n \\n제58조(재해손실세액공제) ① 사업자가 해당 과세기간에 천재지변이나 그 밖의 재해(이하 “재해”라 한다)로 대통령령으\\n로 정하는 자산총액(이하 이 항에서 “자산총액”이라 한다)의 100분의 20 이상에 해당하는 자산을 상실하여 납세가\\n곤란하다고 인정되는 경우에는 다음 각 호의 소득세액(사업소득에 대한 소득세액을 말한다. 이하 이 조에서 같다)에\\n그 상실된 가액이 상실 전의 자산총액에서 차지하는 비율(이하 이 조에서 “자산상실비율”이라 한다)을 곱하여 계산\\n한 금액(상실된 자산의 가액을 한도로 한다)을 그 세액에서 공제한다. 이 경우 자산의 가액에는 토지의 가액을 포함\\n하지 아니한다. <개정 2020. 12. 29.>\\n1. 재해 발생일 현재 부과되지 아니한 소득세와 부과된 소득세로서 미납된 소득세액\\n2. 재해 발생일이 속하는 과세기간의 소득에 대한 소득세액\\n② 제1항의 경우에 제56조, 제56조의2, 제57조 및 제57조의2에 따라 공제할 세액이 있을 때에는 이를 공제한 후의\\n세액을 소득세액으로 하여 제1항을 적용한다.<개정 2022. 12. 31.>\\n③ 제1항에 따른 공제를 “재해손실세액공제”라 한다.\\n④ 재해손실세액공제를 적용받으려는 자는 대통령령으로 정하는 바에 따라 관할 세무서장에게 신청할 수 있다.\\n⑤ 관할 세무서장이 제4항의 신청을 받았을 때에는 그 공제할 세액을 결정하여 신청인에게 알려야 한다.\\n⑥ 제4항의 신청이 없는 경우에도 제1항을 적용한다.\\n⑦ 집단적으로 재해가 발생한 경우에는 대통령령으로 정하는 바에 따라 관할 세무서장이 조사결정한 자산상실비율\\n에 따라 제1항을 적용한다.\\n⑧ 재해손실세액공제에 관하여 필요한 사항은 대통령령으로 정한다.\\n[전문개정 2009. 12. 31.]\\n \\n제59조(근로소득세액공제) ①근로소득이 있는 거주자에 대해서는 그 근로소득에 대한 종합소득산출세액에서 다음의\\n금액을 공제한다. <개정 2014. 1. 1., 2015. 5. 13.>\\n② 제1항에도 불구하고 공제세액이 다음 각 호의 구분에 따른 금액을 초과하는 경우에 그 초과하는 금액은 없는 것\\n으로 한다.<신설 2014. 1. 1., 2015. 5. 13., 2022. 12. 31.>\\n1. 총급여액이 3천 300만원 이하인 경우: 74만원\\n2. 총급여액이 3천 300만원 초과 7천만원 이하인 경우: 74만원 - [(총급여액 - 3천 300만원) × 8/1000]. 다만, 위 금\\n액이 66만원보다 적은 경우에는 66만원으로 한다.\\n3. 총급여액이 7천만원 초과 1억2천만원 이하인 경우: 66만원 - [(총급여액 - 7천만원) × 1/2]. 다만, 위 금액이 50만\\n원보다 적은 경우에는 50만원으로 한다.\\n4. 총급여액이 1억2천만원을 초과하는 경우: 50만원 - [(총급여액 - 1억2천만원) × 1/2]. 다만, 위 금액이 20만원보다\\n적은 경우에는 20만원으로 한다.\\n③ 일용근로자의 근로소득에 대해서 제134조제3항에 따른 원천징수를 하는 경우에는 해당 근로소득에 대한 산출세\\n액의 100분의 55에 해당하는 금액을 그 산출세액에서 공제한다.<개정 2014. 1. 1.>\\n[전문개정 2012. 1. 1.]\\n \\n제59조의2(자녀세액공제) ①종합소득이 있는 거주자의 기본공제대상자에 해당하는 자녀(입양자 및 위탁아동을 포함하\\n며, 이하 이 조에서 “공제대상자녀”라 한다) 및 손자녀로서 8세 이상의 사람에 대해서는 다음 각 호의 구분에 따른 금\\n액을 종합소득산출세액에서 공제한다. <개정 2015. 5. 13., 2017. 12. 19., 2018. 12. 31., 2019. 12. 31., 2022. 12. 31.,\\n2023. 12. 31., 2024. 12. 31.>')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[35] # 원하는 내용이 55조 세율 내용이라면 인덱싱 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43998bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply() # Jupyter 에서 asyncio를 사용할 때 (pyzerox는 event loop가 있으면 작동하지 않음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a0908b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-N9GDqxsHYOw91R7wSv352Bcw on tokens per min (TPM): Limit 60000, Used 60000, Requested 807. Please try again in 807ms. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-N9GDqxsHYOw91R7wSv352Bcw on tokens per min (TPM): Limit 60000, Used 60000, Requested 807. Please try again in 807ms. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-N9GDqxsHYOw91R7wSv352Bcw on tokens per min (TPM): Limit 60000, Used 60000, Requested 807. Please try again in 807ms. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-N9GDqxsHYOw91R7wSv352Bcw on tokens per min (TPM): Limit 60000, Used 60000, Requested 807. Please try again in 807ms. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-N9GDqxsHYOw91R7wSv352Bcw on tokens per min (TPM): Limit 60000, Used 60000, Requested 807. Please try again in 807ms. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-N9GDqxsHYOw91R7wSv352Bcw on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-N9GDqxsHYOw91R7wSv352Bcw on tokens per min (TPM): Limit 60000, Used 60000, Requested 807. Please try again in 807ms. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-N9GDqxsHYOw91R7wSv352Bcw on tokens per min (TPM): Limit 60000, Used 60000, Requested 807. Please try again in 807ms. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-N9GDqxsHYOw91R7wSv352Bcw on tokens per min (TPM): Limit 60000, Used 60000, Requested 807. Please try again in 807ms. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-N9GDqxsHYOw91R7wSv352Bcw on tokens per min (TPM): Limit 60000, Used 60000, Requested 807. Please try again in 807ms. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-N9GDqxsHYOw91R7wSv352Bcw on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-N9GDqxsHYOw91R7wSv352Bcw on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-N9GDqxsHYOw91R7wSv352Bcw on tokens per min (TPM): Limit 60000, Used 60000, Requested 807. Please try again in 807ms. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-N9GDqxsHYOw91R7wSv352Bcw on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-N9GDqxsHYOw91R7wSv352Bcw on tokens per min (TPM): Limit 60000, Used 60000, Requested 807. Please try again in 807ms. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-N9GDqxsHYOw91R7wSv352Bcw on tokens per min (TPM): Limit 60000, Used 60000, Requested 807. Please try again in 807ms. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-N9GDqxsHYOw91R7wSv352Bcw on tokens per min (TPM): Limit 60000, Used 60000, Requested 807. Please try again in 807ms. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-N9GDqxsHYOw91R7wSv352Bcw on tokens per min (TPM): Limit 60000, Used 60000, Requested 807. Please try again in 807ms. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-N9GDqxsHYOw91R7wSv352Bcw on tokens per min (TPM): Limit 60000, Used 60000, Requested 807. Please try again in 807ms. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-N9GDqxsHYOw91R7wSv352Bcw on tokens per min (TPM): Limit 60000, Used 60000, Requested 807. Please try again in 807ms. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-N9GDqxsHYOw91R7wSv352Bcw on tokens per min (TPM): Limit 60000, Used 60000, Requested 807. Please try again in 807ms. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-N9GDqxsHYOw91R7wSv352Bcw on tokens per min (TPM): Limit 60000, Used 60000, Requested 807. Please try again in 807ms. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-N9GDqxsHYOw91R7wSv352Bcw on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-N9GDqxsHYOw91R7wSv352Bcw on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-N9GDqxsHYOw91R7wSv352Bcw on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n",
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-N9GDqxsHYOw91R7wSv352Bcw on tokens per min (TPM): Limit 60000, Used 60000, Requested 807. Please try again in 807ms. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-N9GDqxsHYOw91R7wSv352Bcw on tokens per min (TPM): Limit 60000, Used 60000, Requested 807. Please try again in 807ms. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-N9GDqxsHYOw91R7wSv352Bcw on tokens per min (TPM): Limit 100000, Used 100000, Requested 807. Please try again in 5h48m37.44s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-N9GDqxsHYOw91R7wSv352Bcw on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-N9GDqxsHYOw91R7wSv352Bcw on tokens per min (TPM): Limit 60000, Used 60000, Requested 807. Please try again in 807ms. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-N9GDqxsHYOw91R7wSv352Bcw on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-N9GDqxsHYOw91R7wSv352Bcw on tokens per min (TPM): Limit 100000, Used 100000, Requested 807. Please try again in 5h48m37.44s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-N9GDqxsHYOw91R7wSv352Bcw on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-N9GDqxsHYOw91R7wSv352Bcw on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-N9GDqxsHYOw91R7wSv352Bcw on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process image Error:\n",
      "    Error in Completion Response. Error: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-N9GDqxsHYOw91R7wSv352Bcw on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n",
      "    Please check the status of your model provider API status.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# 메인 함수를 실행합니다:\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m result = \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# 마크다운 결과를 출력합니다\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Study-2/langchain/kang/.venv/lib/python3.13/site-packages/nest_asyncio.py:30\u001b[39m, in \u001b[36m_patch_asyncio.<locals>.run\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m     28\u001b[39m task = asyncio.ensure_future(main)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task.done():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Study-2/langchain/kang/.venv/lib/python3.13/site-packages/nest_asyncio.py:92\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     90\u001b[39m     f._log_destroy_pending = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping:\n\u001b[32m     94\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Study-2/langchain/kang/.venv/lib/python3.13/site-packages/nest_asyncio.py:115\u001b[39m, in \u001b[36m_patch_loop.<locals>._run_once\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    108\u001b[39m     heappop(scheduled)\n\u001b[32m    110\u001b[39m timeout = (\n\u001b[32m    111\u001b[39m     \u001b[32m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[32m    113\u001b[39m         scheduled[\u001b[32m0\u001b[39m]._when - \u001b[38;5;28mself\u001b[39m.time(), \u001b[32m0\u001b[39m), \u001b[32m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;28mself\u001b[39m._process_events(event_list)\n\u001b[32m    118\u001b[39m end_time = \u001b[38;5;28mself\u001b[39m.time() + \u001b[38;5;28mself\u001b[39m._clock_resolution\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.3-linux-x86_64-gnu/lib/python3.13/selectors.py:452\u001b[39m, in \u001b[36mEpollSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    450\u001b[39m ready = []\n\u001b[32m    451\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m452\u001b[39m     fd_event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_ev\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from pyzerox import zerox\n",
    "import asyncio\n",
    "\n",
    "### 모델 설정 (Vision 모델만 사용) 참고: https://docs.litellm.ai/docs/providers ###\n",
    "\n",
    "## 일부 모델에 필요할 수 있는 추가 모델 kwargs의 자리 표시자\n",
    "kwargs = {}\n",
    "\n",
    "## Vision 모델에 사용할 시스템 프롬프트\n",
    "custom_system_prompt = None\n",
    "\n",
    "model = \"gpt-4o-mini\" \n",
    "\n",
    "# 메인 비동기 진입점을 정의합니다\n",
    "async def main():\n",
    "    file_path = \"./docs/income_tax.pdf\" ## 로컬 파일 경로 및 파일 URL 지원\n",
    "\n",
    "    ## 일부 페이지 또는 전체 페이지를 처리\n",
    "    select_pages = None ## 전체는 None, 특정 페이지는 int 또는 list(int) 페이지 번호 (1부터 시작)\n",
    "\n",
    "    output_dir = \"./docs\" ## 통합된 마크다운 파일을 저장할 디렉토리\n",
    "    result = await zerox(file_path=file_path, model=model, output_dir=output_dir,\n",
    "                        custom_system_prompt=custom_system_prompt, select_pages=select_pages, **kwargs)\n",
    "    return result\n",
    "\n",
    "\n",
    "# 메인 함수를 실행합니다:\n",
    "result = asyncio.run(main())\n",
    "\n",
    "# 마크다운 결과를 출력합니다\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09199a55",
   "metadata": {},
   "source": [
    "# pyzerox\n",
    "- 문서 파싱 결과는 MarkDown\n",
    "  - 표도 마크다운으로 정리됨\n",
    "  - 이 결과 MarkDown을 로딩하면됨\n",
    "- 사용 패키지 : \"unstructured[md]\" nltk -> 같이 쓰는 것을 langchain에서 권장\n",
    "- PDF에서는 문서를 page 단위로 쪼갰었음\n",
    "- Markdown은 split을 해줘야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ca1acab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=100,\n",
    "    separators=['\\n\\n', '\\n']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d742eb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "\n",
    "markdown_path = \"./docs/income_tax.md\"\n",
    "loader = UnstructuredMarkdownLoader(markdown_path)\n",
    "document_list = loader.load_and_split(text_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "65c0111b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': './docs/income_tax.md'}, page_content='제2절 세액의 계산 <개정 2009. 12. 31.> 제1관 세율 <개정 2009. 12. 31.>\\n\\n제55조(세율) 거주자의 종합소득에 대한 소득세는 해당 연도의 종합소득과세표준에 다음의 세율을 적용하여 계산한 금액(이하 \"종합소득과세표준세액\"이라 한다)을 그 세액으로 한다. <개정 2014. 1. 1., 2016. 12. 20., 2017. 12. 19., 2020. 12. 29., 2022. 12. 31.>\\n\\n종합소득 ┌───────────────┐ │ 과세표준의 6개 구간 │ ├───────────────┤ │ 1,400만원 이하 │ 84만원 + (1,400만원을 초과하는 금액의 15%) │ │ 1,400만원 초과 │ 84만원 + (5,000만원을 초과하는 금액의 24%) │ │ 8,800만원 이하 │ 624만원 + (5,000만원을 초과하는 금액의 24%) │ │ 8,800만원 초과 │ 1,536만원 + (8,800만원을 초과하는 금액의 35%) │ │ 1.5억원 초과 │ 4,046만원 + (1,500만원을 초과하는 금액의 38%) │ │ 3억원 초과 │ 6,460만원 + (3억원을 초과하는 금액의 40%) │ │ 5억원 초과 │ 14,760만원 + (5억원을 초과하는 금액의 42%) │ │ 10억원 초과 │ 38,406만원 + (10억원을 초과하는 금액의 45%) │ └───────────────┘\\n\\n② 거주자의 퇴직소득에 대한 소득세는 다음 각 호의 순서에 따라 계산한 금액(이하 ‘퇴직소득 산출세액’이라 한다)으로 한다. <개정 2013. 1. 1., 2014. 12. 23.> 1. 해당 과세기간의 퇴직소득과세표준 제1항의 세율을 적용하여 계산한 금액 2. 제1호의 금액을 12로 나눈 금액에 근속연수를 곱한 금액 3. 삭제<2014. 12. 23.> [전문개정 2009. 12. 31.]\\n\\n제2관 세액공제 <개정 2009. 12. 31.>\\n\\n소 제목\\n\\n제56조(배당세율공제)\\n\\n① 거주자의 종합소득금액에 제17조제3항 각 호 외의 부분 단서가 적용되는 배당소득에 함산되어 있는 경우에는 각 호 외의 부분 단서에 대한 해석계기로서 종합소득금액에 대한 금액에 대하여 배당세율공제를 한다. <개정 2009. 12. 31.> ② 제1항에 따른 공제는 \"배당세율공제\"란다. <개정 2009. 12. 31.> ③ 삭제<2003. 12. 30.> ④ 제1항을 적용할 때 배당세액공제의 대상이 되는 배당소득금액은 제14조제3항의 종합소득과세표준에 포함된 배당소득금액으로 이자소득의 종합과세금액을 초과하는 것으로 한다. <개정 2009. 12. 31.> ⑤ 삭제<2006. 12. 30.> ⑥ 배당세액공제의 계산 등에 필요한 사항은 대통령령으로 정한다. <개정 2009. 12. 31.> [제목개정 2009. 12. 31.]\\n\\n제56조의2(기장세액공제)')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_list[40]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6249036e",
   "metadata": {},
   "source": [
    "- markdown의 전처리 결과가 이전보다는 낫지만 표의 내용이 짤리게된다. -> 정확한 Context를 전달하게 되는 것은 아니게됨\n",
    "- md -> text -> load -> split 과정이 필요해짐\n",
    "  - 추가 설치 패키지 : html2text beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94c01205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Markdown converted to plain text successfully!\n"
     ]
    }
   ],
   "source": [
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "text_path = './docs/income_tax.txt'\n",
    "\n",
    "# 마크다운 파일을 읽어옵니다\n",
    "with open(markdown_path, 'r', encoding='utf-8') as md_file:\n",
    "    md_content = md_file.read()\n",
    "\n",
    "# 마크다운 콘텐츠를 HTML로 변환합니다\n",
    "html_content = markdown.markdown(md_content)\n",
    "\n",
    "# HTML 콘텐츠를 파싱하여 텍스트만 추출합니다\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "text_content = soup.get_text()\n",
    "\n",
    "# 추출한 텍스트를 텍스트 파일로 저장합니다\n",
    "with open(text_path, 'w', encoding='utf-8') as txt_file:\n",
    "    txt_file.write(text_content)\n",
    "\n",
    "print(\"Markdown converted to plain text successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c65eded",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader('./docs/income_tax.txt')\n",
    "document_list = loader.load_and_split(text_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "257625e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': './docs/income_tax.txt'}, page_content='② 제70조제1항, 제70조의2에 따른 제74조에 따라 차례로 할 것이 제70조제1항제2호에 따르며 서류를 제출하여야 한다는 경우에는 기준소득 중 거주자 본인이 된다(분산)과 제70조제2와 제74조에 따른 제료 및 제대법을 포함한다. 단, 차별제표청정인 그 업체를 남겨 제출한 경우로 그에 대하여 아니하다.<개정 2013. 1. 1.>\\n  ③ 제80조에 따른 수익과 관련의 경우에는 기초공제 중 거주자 본인이 된다(분산)과 그에 관한 적지사항을 분명히 한다.\\n[전문개정 2009. 12. 31.]\\n[제목개정 2014. 1. 1.]\\n제54조의2(공동사업에 대한 소득공제 등 특례) 제51조의3 또는 「조세특례제한법」에 따른 소득공제를 적용하거나 제59조의2에 따른 세액감면을 적용하는 경우 제54조제3항에 따라 공동사업자의 소득에 합산과세되는 특별세액거래의 지출․납입․투자 등의 금액이 있을 경우 주된 공동사업자의 소득에 합산과세되는 소득금액에 합산되어 주된 공동사업자의 합산과세세액은 공동사업소득액 또는 공동사업창출세액을 계산할 때 소득공제 또는 세액공제를 받을 수 있다. \\n[개정 2014. 1. 1.]\\n[전문개정 2009. 12. 31.]\\n[제목개정 2014. 1. 1.]\\n제2절 세액의 계산 <개정 2009. 12. 31.>\\n제1관 세율 <개정 2009. 12. 31.>\\n제55조(세율) 거주자의 종합소득에 대한 소득세는 해당 연도의 종합소득과세표준에 다음의 세율을 적용하여 계산한 금액(이하 \"종합소득과세표준세액\"이라 한다)을 그 세액으로 한다. <개정 2014. 1. 1., 2016. 12. 20., 2017. 12. 19., 2020. 12. 29., 2022. 12. 31.>\\n종합소득\\n┌───────────────┐\\n│ 과세표준의 6개 구간 │\\n├───────────────┤\\n│ 1,400만원 이하        │ 84만원 + (1,400만원을 초과하는 금액의 15%)  │\\n│ 1,400만원 초과        │ 84만원 + (5,000만원을 초과하는 금액의 24%)  │\\n│ 8,800만원 이하        │ 624만원 + (5,000만원을 초과하는 금액의 24%)  │\\n│ 8,800만원 초과        │ 1,536만원 + (8,800만원을 초과하는 금액의 35%)  │\\n│ 1.5억원 초과          │ 4,046만원 + (1,500만원을 초과하는 금액의 38%)  │\\n│ 3억원 초과            │ 6,460만원 + (3억원을 초과하는 금액의 40%)  │\\n│ 5억원 초과            │ 14,760만원 + (5억원을 초과하는 금액의 42%)  │\\n│ 10억원 초과           │ 38,406만원 + (10억원을 초과하는 금액의 45%)  │\\n└───────────────┘\\n② 거주자의 퇴직소득에 대한 소득세는 다음 각 호의 순서에 따라 계산한 금액(이하 ‘퇴직소득 산출세액’이라 한다)으로 한다. <개정 2013. 1. 1., 2014. 12. 23.>')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_list[47]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d61b6d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1553d04",
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_chroma\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Chroma\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m vector_store = \u001b[43mChroma\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdocument_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m=\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mincome_tax_collection\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./income_tax_collection\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Study-2/langchain/kang/.venv/lib/python3.13/site-packages/langchain_chroma/vectorstores.py:1371\u001b[39m, in \u001b[36mChroma.from_documents\u001b[39m\u001b[34m(cls, documents, embedding, ids, collection_name, persist_directory, host, port, headers, chroma_cloud_api_key, tenant, database, client_settings, client, collection_metadata, collection_configuration, ssl, **kwargs)\u001b[39m\n\u001b[32m   1369\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1370\u001b[39m     ids = [doc.id \u001b[38;5;28;01mif\u001b[39;00m doc.id \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(uuid.uuid4()) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[32m-> \u001b[39m\u001b[32m1371\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1372\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m=\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1374\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1375\u001b[39m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1376\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1377\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1378\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhost\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1379\u001b[39m \u001b[43m    \u001b[49m\u001b[43mport\u001b[49m\u001b[43m=\u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1380\u001b[39m \u001b[43m    \u001b[49m\u001b[43mssl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mssl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1381\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1382\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchroma_cloud_api_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchroma_cloud_api_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1383\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1384\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1385\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1386\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1387\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1388\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_configuration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1389\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1390\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Study-2/langchain/kang/.venv/lib/python3.13/site-packages/langchain_chroma/vectorstores.py:1304\u001b[39m, in \u001b[36mChroma.from_texts\u001b[39m\u001b[34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, host, port, headers, chroma_cloud_api_key, tenant, database, client_settings, client, collection_metadata, collection_configuration, ssl, **kwargs)\u001b[39m\n\u001b[32m   1296\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbatch_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_batches\n\u001b[32m   1298\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m create_batches(\n\u001b[32m   1299\u001b[39m         api=chroma_collection._client,\n\u001b[32m   1300\u001b[39m         ids=ids,\n\u001b[32m   1301\u001b[39m         metadatas=metadatas,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m   1302\u001b[39m         documents=texts,\n\u001b[32m   1303\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1304\u001b[39m         \u001b[43mchroma_collection\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1305\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1306\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1307\u001b[39m \u001b[43m            \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1308\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1309\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1310\u001b[39m     chroma_collection.add_texts(texts=texts, metadatas=metadatas, ids=ids)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Study-2/langchain/kang/.venv/lib/python3.13/site-packages/langchain_chroma/vectorstores.py:606\u001b[39m, in \u001b[36mChroma.add_texts\u001b[39m\u001b[34m(self, texts, metadatas, ids, **kwargs)\u001b[39m\n\u001b[32m    604\u001b[39m texts = \u001b[38;5;28mlist\u001b[39m(texts)\n\u001b[32m    605\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m606\u001b[39m     embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_embedding_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m metadatas:\n\u001b[32m    608\u001b[39m     \u001b[38;5;66;03m# fill metadatas with empty dicts if somebody\u001b[39;00m\n\u001b[32m    609\u001b[39m     \u001b[38;5;66;03m# did not specify metadata for all texts\u001b[39;00m\n\u001b[32m    610\u001b[39m     length_diff = \u001b[38;5;28mlen\u001b[39m(texts) - \u001b[38;5;28mlen\u001b[39m(metadatas)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Study-2/langchain/kang/.venv/lib/python3.13/site-packages/langchain_openai/embeddings/base.py:590\u001b[39m, in \u001b[36mOpenAIEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts, chunk_size, **kwargs)\u001b[39m\n\u001b[32m    587\u001b[39m \u001b[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[32m    588\u001b[39m \u001b[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[32m    589\u001b[39m engine = cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m.deployment)\n\u001b[32m--> \u001b[39m\u001b[32m590\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    592\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Study-2/langchain/kang/.venv/lib/python3.13/site-packages/langchain_openai/embeddings/base.py:478\u001b[39m, in \u001b[36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[39m\u001b[34m(self, texts, engine, chunk_size, **kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m batched_embeddings: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m]] = []\n\u001b[32m    477\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m _iter:\n\u001b[32m--> \u001b[39m\u001b[32m478\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m_chunk_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mclient_kwargs\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    482\u001b[39m         response = response.model_dump()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Study-2/langchain/kang/.venv/lib/python3.13/site-packages/openai/resources/embeddings.py:132\u001b[39m, in \u001b[36mEmbeddings.create\u001b[39m\u001b[34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m             embedding.embedding = np.frombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[32m    127\u001b[39m                 base64.b64decode(data), dtype=\u001b[33m\"\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    128\u001b[39m             ).tolist()\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/embeddings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Study-2/langchain/kang/.venv/lib/python3.13/site-packages/openai/_base_client.py:1256\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1242\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1243\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1244\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1251\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1252\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1253\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1254\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1255\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1256\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Study-2/langchain/kang/.venv/lib/python3.13/site-packages/openai/_base_client.py:1044\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1041\u001b[39m             err.response.read()\n\u001b[32m   1043\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1044\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1046\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1048\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store = Chroma.from_documents(documents=document_list,\n",
    "                                     embedding=embeddings,\n",
    "                                     collection_name=\"income_tax_collection\",\n",
    "                                     persist_directory=\"./income_tax_collection\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ffc3d45b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vector_store' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m retriever = \u001b[43mvector_store\u001b[49m.as_retriever(search_kwargs={\u001b[33m\"\u001b[39m\u001b[33mk\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m3\u001b[39m})\n",
      "\u001b[31mNameError\u001b[39m: name 'vector_store' is not defined"
     ]
    }
   ],
   "source": [
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9c52e3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"연봉 5천만원 직장인의 소득세는 ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a9803770",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retriever' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mretriever\u001b[49m.invoke(query)\n",
      "\u001b[31mNameError\u001b[39m: name 'retriever' is not defined"
     ]
    }
   ],
   "source": [
    "retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d34153d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    query: str\n",
    "    context: list[Document]\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7f18f7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "\n",
    "graph_builder = StateGraph(AgentState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f61e46a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state: AgentState):\n",
    "    query = state[\"query\"]\n",
    "    docs = retriever.invoke(query)\n",
    "    \n",
    "    return {\"context\": docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7ace0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "import os\n",
    "def genereate(state: AgentState):\n",
    "    client = Client(api_key=os.getenv(\"LANGSMITH_API_KEY\"))\n",
    "    prompt = client.pull_prompt(\"rlm/rag-prompt\", include_model=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
